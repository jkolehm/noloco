# Model size and learning parameters are inspired from https://github.com/karpathy/llama2.c 
# and Super Tiny Language Models paper https://arxiv.org/pdf/2405.14159v1
defaults:
  - _self_
  - c4_base_7b

exp_name: c4_fsdp_7b
dp_world_size: 16
batch_size: 128
log_dir: /home/gensyn/shared/jari/experiments

optimizer:
  _target_: noloco.dp_optimizer.DataParallelOptimizerBuilder
  lr: 1.2e-4
